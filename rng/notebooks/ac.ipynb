{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting gym\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ab/b1/eb05a423eb801ab7d0715d6a3b28d92589e30b437052553df19ca2087240/gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/amax/anaconda3/envs/zecheng/lib/python3.10/site-packages (from gym) (1.26.2)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/43/dae06432d0c4b1dc9e9149ad37b4ca8384cf6eb7700cd9215b177b914f0a/cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/25/26/d786c6bec30fe6110fd3d22c9a273a2a0e56c0b73b93e25ea1af5a53243b/gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827621 sha256=f4f7cd65308d2c763304d9bc3ae6afdd0af25d0faf94d323f12eff60d871fd4e\n",
      "  Stored in directory: /home/amax/.cache/pip/wheels/77/59/be/835c44599292a3abed875fc29b60dd489b8df8bc69c6103970\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CartPoleEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m env \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39munwrapped\n\u001b[0;32m---> 18\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv.action_space :\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CartPoleEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma = 0.98\n",
    "n_rollout = 10\n",
    "MAX_EPISODE = 10000\n",
    "RENDER = True\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped\n",
    "# env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "print(\"env.action_space :\", env.action_space)\n",
    "print(\"env.observation_space :\", env.observation_space)\n",
    "\n",
    "n_features = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, n_features=n_features, n_actions=n_actions, learning_rate=learning_rate, ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.data = []\n",
    "        hidden_dims = 256\n",
    "        \n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(n_features, hidden_dims),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_pi = nn.Linear(hidden_dims, n_actions)\n",
    "        self.fc_v = nn.Linear(hidden_dims, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def pi(self, x):\n",
    "        x = self.feature_layer(x)\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=-1)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = self.feature_layer(x)\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "    \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        \"\"\"\n",
    "        s: state\n",
    "        a: action pi(a | s, theta)\n",
    "        r: reward (value function)\n",
    "        s_: next state\n",
    "        \"\"\"\n",
    "        s_lst, a_lst, r_lst, s_next_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r / 100.0])\n",
    "            s_next_lst.append(s_)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_lst.append([done_mask])\n",
    "\n",
    "        s_batch, a_batch, r_batch, s_next_batch, done_batch = \\\n",
    "            torch.tensor(np.array(s_lst), dtype=torch.float), \\\n",
    "            torch.tensor(a_lst), \\\n",
    "            torch.tensor(np.array(r_lst), dtype=torch.float), \\\n",
    "            torch.tensor(np.array(s_next_lst), dtype=torch.float), \\\n",
    "            torch.tensor(np.array(done_lst), dtype=torch.float)\n",
    "            \n",
    "        self.data = []\n",
    "        return s_batch, a_batch, r_batch, s_next_batch, done_batch\n",
    "\n",
    "    def train_net(self):\n",
    "        s, a, r, s_, done = self.make_batch()\n",
    "        td_target = r + gamma * self.v(s_) * done\n",
    "        delta = td_target - self.v(s)\n",
    "\n",
    "        pi = self.pi(s)\n",
    "        pi_a = pi.gather(1, a)\n",
    "        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "def main():\n",
    "    model = ActorCritic()\n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "\n",
    "    for n_epi in range(MAX_EPISODE):\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            for t in range(n_rollout):\n",
    "                if RENDER:\n",
    "                    env.render()\n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_next, r, done, info = env.step(a)\n",
    "                model.put_data((s, a, r, s_next, done))\n",
    "\n",
    "                s = s_next\n",
    "                score += r\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score / print_interval))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.classic_control.cartpole.CartPoleEnv at 0x7f340b74f640>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
